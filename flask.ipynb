{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Drive predictable B2B revenue growth with insights from big data and CDPs', 'Daily Crunch: Shopify confirms data breach', 'How to Piss Off Advertisers With Your iOS 14 Settings', 'Transposit scores $35M to build data-driven runbooks for faster disaster recovery', 'Big tech has 2 elephants in the room: Privacy and competition']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def NewsFromBBC(): \n",
    "\n",
    "    # BBC news api \n",
    "    main_url = \"https://newsapi.org/v2/everything?\"\n",
    "    parameters = {\n",
    "    'q': 'big data', # query phrase\n",
    "    'pageSize': 5,  # maximum is 100\n",
    "    'apiKey': '38c5b69ec17c41b5aa61f1d6467f18ef' # your own API key\n",
    "    }\n",
    "    # fetching data in json format \n",
    "    open_bbc_page = requests.get(main_url, params=parameters).json() \n",
    "\n",
    "    # getting all articles in a string article \n",
    "    article = open_bbc_page[\"articles\"] \n",
    "\n",
    "    # empty list which will\n",
    "    # contain all trending news \n",
    "    results = [] \n",
    "\n",
    "    for ar in article: \n",
    "        results.append(ar[\"title\"]) \n",
    "\n",
    "#     for i in range(len(results)): \n",
    "\n",
    "#         # printing all trending news \n",
    "#         print(i + 1, results[i])\n",
    "\n",
    "    return results\n",
    "\n",
    "# Driver Code \n",
    "# if name == 'main': \n",
    "\n",
    "    # function call \n",
    "results = NewsFromBBC() \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_embedd(results):\n",
    "    df = pd.DataFrame(results)\n",
    "    df[0] = df[0].str.split(' ').apply(lambda x: ' '.join([k for k in x if not (('http://' in k) or ('.com'  in k) or ('@'  in k) or ('#'  in k))]))\n",
    "    df[0] = df[0].str.replace(r'[^a-zA-Z\\s]','').str.lower()\n",
    "    test_data = df[0].apply(lambda x: get_embedding(x))\n",
    "    X_test = []\n",
    "    for index in range(len(test_data)):\n",
    "        X_test.append(test_data[index])\n",
    "    \n",
    "    X_test = np.array(X_test)\n",
    "    return X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict1(X_test):\n",
    "    classes = model.predict(X_test)\n",
    "    return np.argmax(classes, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'http://localhost:5000/predict'\n",
    "r = requests.post(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
