{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-09-26 17:36:56--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2020-09-26 17:36:57--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2020-09-26 17:36:58--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  1.88MB/s    in 8m 24s  \n",
      "\n",
      "2020-09-26 17:45:23 (1.63 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /home/lenovo/Documents/Hackathon/glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n"
     ]
    }
   ],
   "source": [
    "!unzip /home/lenovo/Documents/Hackathon/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/kaggle/working/glove.6B.zip': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "#!rm /kaggle/working/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Using cached https://files.pythonhosted.org/packages/5c/4e/afe2315e08a38967f8a3036bbe7e38b428e9b7a90e823a83d0d49df1adf5/gensim-3.8.3-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /home/lenovo/anaconda3/lib/python3.7/site-packages (from gensim) (1.3.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /home/lenovo/anaconda3/lib/python3.7/site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /home/lenovo/anaconda3/lib/python3.7/site-packages (from gensim) (1.17.2)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/b2/97/453576cd8e57cb4f237dbf4dd91097bd26842068661c78a18d61a5ca9f70/smart_open-2.2.0.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: requests in /home/lenovo/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/lenovo/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (1.24.2)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /home/lenovo/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /home/lenovo/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /home/lenovo/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smart-open: filename=smart_open-2.2.0-cp37-none-any.whl size=116513 sha256=1dd22be6e9b3af4851ed3f0e08ca976a77d3cad963c9d7fefee4aee96c87d35b\n",
      "  Stored in directory: /home/lenovo/.cache/pip/wheels/03/04/55/a6c593190f6852d2affb744e398a8fde5b5a63cd0c7e005b83\n",
      "Successfully built smart-open\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-3.8.3 smart-open-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "glove2word2vec(glove_input_file=\"/home/lenovo/Documents/Hackathon/glove.6B/glove.6B.100d.txt\", word2vec_output_file=\"gensim_glove_vectors.txt\")\n",
    "glove_model = KeyedVectors.load_word2vec_format(\"gensim_glove_vectors.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map_dict = {\"positive\": 4, \"negative\":0}\n",
    "# financial_news_data = pd.read_csv(\"../input/sentiment-analysis-for-financial-news/all-data.csv\", encoding=\"ISO 8859-1\", header=None)\n",
    "# financial_news_data[0] = financial_news_data[0].map(map_dict)\n",
    "twitter_data = pd.read_csv(\"/home/lenovo/Documents/Hackathon/archive/training.1600000.processed.noemoticon.csv\", encoding=\"ISO 8859-1\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0           1                             2         3                4  \\\n",
      "0  0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n",
      "1  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
      "2  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
      "3  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
      "4  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
      "\n",
      "                                                   5  \n",
      "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
      "1  is upset that he can't update his Facebook by ...  \n",
      "2  @Kenichan I dived many times for the ball. Man...  \n",
      "3    my whole body feels itchy and like its on fire   \n",
      "4  @nationwideclass no, it's not behaving at all....  \n"
     ]
    }
   ],
   "source": [
    "print(twitter_data.head())\n",
    "# print(financial_news_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data = twitter_data[[0, 5]]\n",
    "twitter_data.rename({0:\"sentiment\", 5:\"text\"}, axis=1, inplace=True)\n",
    "# financial_news_data.rename({0:\"sentiment\", 1:\"text\"}, axis=1, inplace=True)\n",
    "\n",
    "# twitter_data = pd.concat([twitter_data, financial_news_data], axis=0)\n",
    "\n",
    "twitter_data.reset_index(inplace=True)\n",
    "\n",
    "twitter_data[\"text\"] = twitter_data[\"text\"].str.split(' ').apply(lambda x: ' '.join([k for k in x if not (('http://' in k) or ('.com'  in k) or ('@'  in k) or ('#'  in k))]))\n",
    "twitter_data[\"text\"] = twitter_data[\"text\"].str.replace(r'[^a-zA-Z\\s]','').str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = glove_model.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SENTENCE EMBEDDING\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def gen_mean(vals, p):\n",
    "    p = float(p)\n",
    "    return np.power(\n",
    "        np.mean(\n",
    "            np.power(\n",
    "                np.array(vals, dtype=complex),\n",
    "                p),\n",
    "            axis=0),\n",
    "        1 / p\n",
    "    )\n",
    "\n",
    "\n",
    "operations = dict([\n",
    "    ('mean', (lambda word_embeddings: [np.mean(word_embeddings, axis=0)], lambda embeddings_size: embeddings_size)),\n",
    "    ('max', (lambda word_embeddings: [np.max(word_embeddings, axis=0)], lambda embeddings_size: embeddings_size)),\n",
    "    ('min', (lambda word_embeddings: [np.min(word_embeddings, axis=0)], lambda embeddings_size: embeddings_size)),\n",
    "    ('p_mean_2', (lambda word_embeddings: [gen_mean(word_embeddings, p=2.0).real], lambda embeddings_size: embeddings_size)),\n",
    "    ('p_mean_3', (lambda word_embeddings: [gen_mean(word_embeddings, p=3.0).real], lambda embeddings_size: embeddings_size)),\n",
    "])\n",
    "\n",
    "\n",
    "def get_sentence_embedding(sentence, embeddings, chosen_operations):\n",
    "    word_embeddings = []\n",
    "    for tok in sentence:\n",
    "        if tok not in vocab:\n",
    "            continue\n",
    "        vec = embeddings[tok]\n",
    "        if vec is not None:\n",
    "            word_embeddings.append(vec)\n",
    "\n",
    "    if not word_embeddings:\n",
    "        sentence_embedding = np.zeros(300)\n",
    "    else:\n",
    "        concat_embs = []\n",
    "        for o in chosen_operations:\n",
    "            concat_embs += operations[o][0](word_embeddings)\n",
    "        sentence_embedding = np.concatenate(\n",
    "            concat_embs,\n",
    "            axis=0\n",
    "        )\n",
    "\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(sent):\n",
    "    return get_sentence_embedding(sent.split(), glove_model, ['min', 'mean', 'max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data = twitter_data[[\"sentiment\", \"text\"]]\n",
    "twitter_data[\"text\"] = twitter_data[\"text\"].apply(lambda x: get_embedding(x))\n",
    "X_train_val = []\n",
    "for index in range(len(twitter_data)):\n",
    "    X_train_val.append(twitter_data[\"text\"][index])\n",
    "    \n",
    "X_train_val = np.array(X_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_val = []\n",
    "for index in range(len(twitter_data)):\n",
    "    y_train_val.append(twitter_data[\"sentiment\"][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"X_train.npy\", X_train_val)\n",
    "# np.save(\"y_train.npy\", np.array(y_train_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_val, y_train, y_val = train_test_split(twitter_data[\"text\"], twitter_data[\"sentiment\"], test_size=0.25, random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 1, 1], dtype=int16)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.array(y_train, dtype=np.int16)\n",
    "y_train[y_train>2] = 1\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_y_train = keras.utils.to_categorical(y_train, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d474fce34333>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m model.compile(loss='categorical_crossentropy',\n\u001b[0m\u001b[1;32m     23\u001b[0m               \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m               metrics=['accuracy'])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "input_shape = (32, 900)\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # model.add(layers.Conv2D(filters=6, kernel_size=(3, 1), strides=(1,1), activation='relu', input_shape=input_shape))\n",
    "    # model.add(layers.AveragePooling2D())\n",
    "\n",
    "    # model.add(layers.Conv2D(filters=16, kernel_size=(3, 1), strides=(1,1), activation='relu'))\n",
    "    # model.add(layers.AveragePooling2D())\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(units=900, activation='relu'))\n",
    "    model.add(layers.Dense(units=256, activation='relu'))\n",
    "    model.add(layers.Dense(units=32, activation='relu'))\n",
    "    model.add(layers.Dense(units=2, activation = 'softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1200000 samples\n",
      "Epoch 1/5\n",
      "1200000/1200000 [==============================] - 500s 417us/sample - loss: 0.5400 - accuracy: 0.7223\n",
      "Epoch 2/5\n",
      "1200000/1200000 [==============================] - 522s 435us/sample - loss: 0.5126 - accuracy: 0.7434\n",
      "Epoch 3/5\n",
      "1200000/1200000 [==============================] - 534s 445us/sample - loss: 0.5028 - accuracy: 0.7501\n",
      "Epoch 4/5\n",
      "1200000/1200000 [==============================] - 543s 453us/sample - loss: 0.4960 - accuracy: 0.7546\n",
      "Epoch 5/5\n",
      "1200000/1200000 [==============================] - 540s 450us/sample - loss: 0.4910 - accuracy: 0.7578\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9956382e50>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(np.array(X_train, dtype=np.float32), cat_y_train, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"twitter_sentiment_100d.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Drive predictable B2B revenue growth with insights from big data and CDPs', 'Daily Crunch: Shopify confirms data breach', 'How to Piss Off Advertisers With Your iOS 14 Settings', 'Transposit scores $35M to build data-driven runbooks for faster disaster recovery', 'Big tech has 2 elephants in the room: Privacy and competition']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def NewsFromBBC(): \n",
    "\n",
    "    # BBC news api \n",
    "    main_url = \"https://newsapi.org/v2/everything?\"\n",
    "    parameters = {\n",
    "    'q': 'big data', # query phrase\n",
    "    'pageSize': 5,  # maximum is 100\n",
    "    'apiKey': '38c5b69ec17c41b5aa61f1d6467f18ef' # your own API key\n",
    "    }\n",
    "    # fetching data in json format \n",
    "    open_bbc_page = requests.get(main_url, params=parameters).json() \n",
    "\n",
    "    # getting all articles in a string article \n",
    "    article = open_bbc_page[\"articles\"] \n",
    "\n",
    "    # empty list which will\n",
    "    # contain all trending news \n",
    "    results = [] \n",
    "\n",
    "    for ar in article: \n",
    "        results.append(ar[\"title\"]) \n",
    "\n",
    "#     for i in range(len(results)): \n",
    "\n",
    "#         # printing all trending news \n",
    "#         print(i + 1, results[i])\n",
    "\n",
    "    return results\n",
    "\n",
    "# Driver Code \n",
    "# if name == 'main': \n",
    "\n",
    "    # function call \n",
    "results = NewsFromBBC() \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Drive predictable B2B revenue growth with insi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Daily Crunch: Shopify confirms data breach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to Piss Off Advertisers With Your iOS 14 S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Transposit scores $35M to build data-driven ru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Big tech has 2 elephants in the room: Privacy ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  Drive predictable B2B revenue growth with insi...\n",
       "1         Daily Crunch: Shopify confirms data breach\n",
       "2  How to Piss Off Advertisers With Your iOS 14 S...\n",
       "3  Transposit scores $35M to build data-driven ru...\n",
       "4  Big tech has 2 elephants in the room: Privacy ..."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(results) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0] = df[0].str.split(' ').apply(lambda x: ' '.join([k for k in x if not (('http://' in k) or ('.com'  in k) or ('@'  in k) or ('#'  in k))]))\n",
    "df[0] = df[0].str.replace(r'[^a-zA-Z\\s]','').str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>drive predictable bb revenue growth with insig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>daily crunch shopify confirms data breach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how to piss off advertisers with your ios  set...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>transposit scores m to build datadriven runboo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>big tech has  elephants in the room privacy an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  drive predictable bb revenue growth with insig...\n",
       "1          daily crunch shopify confirms data breach\n",
       "2  how to piss off advertisers with your ios  set...\n",
       "3  transposit scores m to build datadriven runboo...\n",
       "4  big tech has  elephants in the room privacy an..."
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = df[0].apply(lambda x: get_embedding(x))\n",
    "X_test = []\n",
    "for index in range(len(test_data)):\n",
    "    X_test.append(test_data[index])\n",
    "    \n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(classes, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive predictable bb revenue growth with insights from big data and cdps\n",
      "daily crunch shopify confirms data breach\n",
      "how to piss off advertisers with your ios  settings\n",
      "transposit scores m to build datadriven runbooks for faster disaster recovery\n",
      "big tech has  elephants in the room privacy and competition\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(df)):\n",
    "    print(df[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_embedd(results):\n",
    "    df = pd.DataFrame(results)\n",
    "    df[0] = df[0].str.split(' ').apply(lambda x: ' '.join([k for k in x if not (('http://' in k) or ('.com'  in k) or ('@'  in k) or ('#'  in k))]))\n",
    "    df[0] = df[0].str.replace(r'[^a-zA-Z\\s]','').str.lower()\n",
    "    test_data = df[0].apply(lambda x: get_embedding(x))\n",
    "    X_test = []\n",
    "    for index in range(len(test_data)):\n",
    "        X_test.append(test_data[index])\n",
    "    \n",
    "    X_test = np.array(X_test)\n",
    "    return X_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict1(X_test):\n",
    "    classes = model.predict(X_test)\n",
    "    return np.argmax(classes, axis=1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "# pipeline = Pipeline(steps= [('embedding', pre_embedd(results)),\n",
    "#                             ('model', predict(X_test))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "import pickle\n",
    "\n",
    "app = Flask(__name__)\n",
    "model = keras.models.load_model(\"twitter_sentiment_100d.h5\")\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/predict',methods=['POST'])\n",
    "def predict():\n",
    "    results = NewsFromBBC() \n",
    "    X_test = pre_embedd(results)\n",
    "    predicted = predict1(X_test)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#     int_features = [int(x) for x in request.form.values()]\n",
    "#     final_features = [np.array(int_features)]\n",
    "#     prediction = model.predict(final_features)\n",
    "\n",
    "#     output = round(prediction[0], 2)\n",
    "\n",
    "    return render_template('index.html', prediction_text='Sales should be $ {}'.format(predicted))\n",
    "\n",
    "#@app.route('/results',methods=['POST'])\n",
    "# def results():\n",
    "\n",
    "#     data = request.get_json(force=True)\n",
    "#     prediction = model.predict([np.array(list(data.values()))])\n",
    "\n",
    "#     output = prediction[0]\n",
    "#     return jsonify(output)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = np.array(y_val, dtype=np.int16)\n",
    "y_val[y_val>2.5] = 1\n",
    "\n",
    "cat_y_val = keras.utils.to_categorical(y_val, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_and_metrics = model.evaluate(X_val, cat_y_val, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_and_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = pd.read_csv(\"/kaggle/input/million-headlines/abcnews-date-text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data[\"headline_text\"][:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = news_data[\"headline_text\"][:32].apply(lambda x: get_embedding(x))\n",
    "X_test = []\n",
    "for index in range(len(test_data)):\n",
    "    X_test.append(test_data[index])\n",
    "    \n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = model.predict(X_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(classes, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for \n",
    "news_data[\"headline_text\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(model.predict(X_val[:32], batch_size=32), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data = pd.read_csv(\"../input/sentiment140/training.1600000.processed.noemoticon.csv\", encoding=\"ISO 8859-1\", header=None, nrows=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data = twitter_data[[0, 5]]\n",
    "twitter_data.rename({0:\"sentiment\", 5:\"text\"}, axis=1, inplace=True)\n",
    "\n",
    "twitter_data.reset_index(inplace=True)\n",
    "\n",
    "twitter_data[\"text\"] = twitter_data[\"text\"].str.split(' ').apply(lambda x: ' '.join([k for k in x if not (('http://' in k) or ('.com'  in k) or ('@'  in k) or ('#'  in k))]))\n",
    "twitter_data[\"text\"] = twitter_data[\"text\"].str.replace(r'[^a-zA-Z\\s]','').str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in twitter_data[\"text\"]:\n",
    "    print(text)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data[\"text\"][:32].apply(lambda x: get_embedding(x))\n",
    "X_test = []\n",
    "for index in range(len(test_data)):\n",
    "    X_test.append(test_data[index])\n",
    "    \n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(model.predict(X_val[:32], batch_size=32), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
